/**
 * Unified Chat API - intelligent routing between stats and expert content
 * NOW WITH HYBRID SUPPORT + STREAMING! üî•
 * Usage: POST /api/chat-unified with { message, history }
 */

import { NextRequest, NextResponse } from 'next/server';
import { Pinecone } from '@pinecone-database/pinecone';
import OpenAI from 'openai';

const openai = new OpenAI({ apiKey: process.env.OPENAI_API_KEY });
const pinecone = new Pinecone({ apiKey: process.env.PINECONE_API_KEY });
const indexName = process.env.PINECONE_INDEX_NAME || 'ed-volley';

// Vector store for expert content
const { searchSimilar } = require('../../../lib/vectorStore');

/**
 * Classify query type using AI - NOW WITH HYBRID! üéØ
 */
async function classifyQuery(message: string): Promise<'stats' | 'expert' | 'hybrid'> {
  try {
    const response = await openai.chat.completions.create({
      model: 'gpt-4o-mini',
      messages: [
        {
          role: 'system',
          content: `Classify volleyball queries into 3 types:

1. "stats" - Pure numbers/statistics request
   Examples: 
   - "Poka≈º statystyki Smarzek"
   - "Ile punkt√≥w zdoby≈Ç Leon"
   - "Top scorers w lidze"

2. "expert" - Pure knowledge/technique/tactics
   Examples:
   - "Co to rotacja 1"
   - "Jak poprawiƒá timing ataku"
   - "Zasady gry w siatk√≥wce"

3. "hybrid" - Requires BOTH stats AND explanation
   Examples:
   - "Dlaczego Smarzek lepsza w playoff"
   - "Por√≥wnaj Leon i Kurek jako atakujƒÖcych"
   - "Kt√≥ry zesp√≥≈Ç ma najlepszy blok i dlaczego"
   - "Wyja≈õnij skuteczno≈õƒá Efimienko w tym sezonie"

Respond ONLY with one word: stats, expert, or hybrid`
        },
        {
          role: 'user',
          content: message
        }
      ],
      temperature: 0.1,
      max_tokens: 10
    });

    const classification = response.choices[0].message.content?.trim().toLowerCase();
    
    if (classification === 'hybrid') return 'hybrid';
    if (classification === 'stats') return 'stats';
    return 'expert';
    
  } catch (error) {
    console.error('Classification error:', error);
    return 'expert'; // Safe fallback
  }
}

/**
 * Search stats using Pinecone
 */
async function searchStats(message: string, limit: number = 5) {
  try {
    // Create embedding
    const embeddingResponse = await openai.embeddings.create({
      model: 'text-embedding-3-small',
      input: message,
      dimensions: 768
    });
    const queryEmbedding = embeddingResponse.data[0].embedding;

    // Search Pinecone
    const index = pinecone.index(indexName);
    const queryResponse = await index.query({
      vector: queryEmbedding,
      topK: limit,
      includeMetadata: true
    });

    console.log(`üîç Stats search: found ${queryResponse.matches?.length || 0} results`);

    return queryResponse.matches?.map(match => ({
      content: match.metadata?.content || '',
      score: match.score || 0,
      source: 'stats',
      metadata: {
        filename: match.metadata?.filename,
        type: match.metadata?.type,
        originalFile: match.metadata?.originalFile
      }
    })) || [];
  } catch (error) {
    console.error('Stats search error:', error);
    return [];
  }
}

/**
 * Search expert content using vectorStore
 */
async function searchExpertContent(message: string, limit: number = 5) {
  try {
    const results = await searchSimilar(message, limit);
    console.log(`üéì Expert search: found ${results?.length || 0} results`);
    
    return results.map((doc: any) => ({
      content: doc.content || doc.pageContent || doc.text || '',
      score: doc.score || doc.similarity || 0,
      source: 'expert',
      metadata: doc.metadata || {}
    }));
  } catch (error) {
    console.error('Expert content search error:', error);
    return [];
  }
}

/**
 * Generate STREAMING response using retrieved context üî•
 */
async function generateResponse(
  message: string,
  context: any[],
  queryType: 'stats' | 'expert' | 'hybrid',
  history: any[] = []
) {
  const contextText = context
    .map((doc, i) => {
      const sourceTag = doc.source === 'stats' ? '[STATS]' : '[EXPERT]';
      return `${sourceTag} [${i + 1}] ${doc.content}`;
    })
    .join('\n\n');

  console.log(`üìù Context length: ${contextText.length} chars`);
  console.log(`üìù Query type: ${queryType}`);

  let systemPrompt;

  if (queryType === 'hybrid') {
    systemPrompt = `Jeste≈õ ekspertem od siatk√≥wki z dostƒôpem do dw√≥ch ≈∫r√≥de≈Ç:
- STATYSTYK graczy i mecz√≥w (oznaczone [STATS])
- WIEDZY EKSPERCKIEJ o taktyce, technice i treningach (oznaczone [EXPERT])

OdpowiadajƒÖc na pytania wymagajƒÖce obu ≈∫r√≥de≈Ç ZAWSZE:
1. Zacznij od KONKRETNYCH LICZB ze statystyk [STATS]
2. Nastƒôpnie WYJA≈öNIJ "dlaczego" u≈ºywajƒÖc wiedzy eksperckiej [EXPERT]
3. PO≈ÅƒÑCZ oba ≈∫r√≥d≈Ça w sp√≥jnƒÖ, naturalnƒÖ odpowied≈∫
4. NIE oznaczaj ≈∫r√≥de≈Ç w odpowiedzi (user ich nie widzi)

‚ö†Ô∏è KRYTYCZNE - ZERO HALLUCINATION:
- U≈ºywaj TYLKO liczb kt√≥re widzisz w kontek≈õcie [STATS]
- Je≈õli NIE MA breakdown (np. playoff vs regular), POWIEDZ TO wprost
- NIE wymy≈õlaj statystyk kt√≥rych nie ma w danych
- Lepiej powiedzieƒá "nie mam oddzielnych danych" ni≈º zgadywaƒá

Kontekst (u≈ºywaj OBA ≈∫r√≥d≈Ça!):
${contextText}

Odpowiadaj po polsku, zwiƒô≈∫le i konkretnie. NIGDY nie wymy≈õlaj liczb.`;

  } else if (queryType === 'stats') {
    systemPrompt = `Jeste≈õ ekspertem od statystyk siatkarskich. Odpowiadaj konkretnie, podajƒÖc liczby i fakty.
U≈ºywaj kontekstu poni≈ºej do odpowiedzi:

${contextText}

Odpowiadaj po polsku, zwiƒô≈∫le i konkretnie. Zawsze podawaj ≈∫r√≥d≈Ço danych je≈õli jest dostƒôpne.`;

  } else {
    systemPrompt = `Jeste≈õ ekspertem od siatk√≥wki - taktyki, techniki i trening√≥w. 
U≈ºywaj kontekstu poni≈ºej do odpowiedzi:

${contextText}

Odpowiadaj po polsku, merytorycznie i praktycznie. Odwo≈Çywaj siƒô do kontekstu gdy to mo≈ºliwe.`;
  }

  const messages: any[] = [
    { role: 'system', content: systemPrompt },
    ...history,
    { role: 'user', content: message }
  ];

  const stream = await openai.chat.completions.create({
    model: 'gpt-4o-mini',
    messages,
    temperature: 0.7,
    max_tokens: 1000,
    stream: true  // ‚Üê STREAMING ENABLED!
  });

  return stream; // Return stream object!
}

export async function POST(request: NextRequest) {
  try {
    const { message, history = [] } = await request.json();

    if (!message) {
      return NextResponse.json({ error: 'Message is required' }, { status: 400 });
    }

    console.log('üì® Unified Chat - New message:', message.substring(0, 50));

    // Step 1: Classify query
    const queryType = await classifyQuery(message);
    console.log(`üîç Query classified as: ${queryType}`);

    // Step 2: Search appropriate source(s)
    let context: any[] = [];

    if (queryType === 'hybrid') {
      console.log('üî• HYBRID MODE - Querying both stats and expert...');
      
      const [statsResults, expertResults] = await Promise.all([
        searchStats(message, 3),
        searchExpertContent(message, 2)
      ]);

      context = [...statsResults, ...expertResults];
      console.log(`‚úÖ Hybrid results: ${statsResults.length} stats + ${expertResults.length} expert = ${context.length} total`);

    } else if (queryType === 'stats') {
      context = await searchStats(message, 5);
    } else {
      context = await searchExpertContent(message, 5);
    }

    console.log(`üìö Found ${context.length} relevant documents`);

    if (context.length === 0) {
      return NextResponse.json({
        response: 'Przepraszam, nie znalaz≈Çem odpowiednich informacji w bazie danych.',
        queryType,
        sources: []
      });
    }

    // Step 3: Generate STREAMING response
    const stream = await generateResponse(message, context, queryType, history);

    // ‚úÖ Create streaming response
    const encoder = new TextEncoder();
    const readable = new ReadableStream({
      async start(controller) {
        try {
          // Send metadata first
          const metadata = {
            queryType,
            sources: context.slice(0, 3).map((doc, i) => ({
              id: i + 1,
              content: doc.content.substring(0, 200) + '...',
              score: doc.score,
              source: doc.source
            }))
          };
          
          controller.enqueue(encoder.encode(`data: ${JSON.stringify({ metadata })}\n\n`));

          // Stream response chunks
          for await (const chunk of stream) {
            const content = chunk.choices[0]?.delta?.content || '';
            if (content) {
              controller.enqueue(encoder.encode(`data: ${JSON.stringify({ content })}\n\n`));
            }
          }
          
          controller.enqueue(encoder.encode('data: [DONE]\n\n'));
          controller.close();
        } catch (error) {
          console.error('Streaming error:', error);
          controller.error(error);
        }
      }
    });

    return new Response(readable, {
      headers: {
        'Content-Type': 'text/event-stream',
        'Cache-Control': 'no-cache',
        'Connection': 'keep-alive',
      },
    });

  } catch (error) {
    console.error('‚ùå Unified chat error:', error);
    return NextResponse.json(
      { 
        error: 'Internal server error',
        details: error instanceof Error ? error.message : 'Unknown error'
      },
      { status: 500 }
    );
  }
}